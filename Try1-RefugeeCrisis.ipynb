{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Records \n",
      "1149\n",
      "Records count with important categories:\n",
      "486\n",
      "Records count after cleaning keywords:\n",
      "396\n",
      "Records count after removing zero-engagment page:\n",
      "359\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b2fa8c66259c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0midlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"filtered_syrian_pages_ids_only.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#important_pages.reset_index().to_json(filepath+\"filtered_syrian_pages.json\",orient='records')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "#from pandas import Series,DataFrame\n",
    "\n",
    "pd.set_option('display.max_rows', 500) \n",
    "filepath = '/Users/Suad/FB-data/'\n",
    "#frame = pd.read_csv(filepath+'Syriapages.csv')\n",
    "frame = pd.read_json(filepath+'Syria-FB-pages.json')\n",
    "print \"loaded Records \"\n",
    "print frame.name.count()\n",
    "\n",
    "slicee = DataFrame(frame)\n",
    "#[['name','category','talking_about_count','engagement','likes','start_info_clean']]\n",
    "#print slicee\n",
    "main_cat = ['Society/Culture Website', 'Travel/Leisure' ,'Transport/Freight', 'Work Position', 'Community', 'Small Business'\n",
    "            , 'Society/Culture Website', 'Business Person', 'Business Services','Cause', 'Community Organization', \n",
    "            'Community/Government' , 'Company' , 'Consulting/Business Services', 'Regional Website', 'Media/News/Publishing'\n",
    "             ,'Local/Travel Website', 'Local Business']\n",
    "important_pages= DataFrame()\n",
    "#columns=['name','category','talking_about_count','engagement','likes','start_info_clean']\n",
    "#print important_pages.columns\n",
    "for cat in main_cat:\n",
    "    #print cat\n",
    "    #print \"--------\"\n",
    "    temp = slicee[slicee.category == cat]\n",
    "    #print temp.name.count()\n",
    "    #print temp.name\n",
    "    important_pages = important_pages.append(temp) \n",
    "\n",
    "print \"Records count with important categories:\"\n",
    "print important_pages.name.count()\n",
    "important_pages = important_pages[~important_pages.name.str.contains(u'عشاق|دوري|كرة|عشق|عاشق|الله|أبطال|فتيات|اسلام|ابطال|بنات')]\n",
    "print \"Records count after cleaning keywords:\"\n",
    "print important_pages.name.count()\n",
    "important_pages = important_pages[important_pages.likes!=0]\n",
    "print \"Records count after removing zero-engagment page:\"\n",
    "print important_pages.name.count()\n",
    "\n",
    "\n",
    "# sort \n",
    "important_pages = important_pages.sort_values(by=['start_info_clean','talking_about_count','likes'], ascending=[1,0,0]) \n",
    "idlist=[]\n",
    "for i in important_pages.id:\n",
    "    if(i>0):\n",
    "        idlist.append(int(i))\n",
    "f = open(filepath+\"filtered_syrian_pages_ids_only.txt\", \"w\")\n",
    "f.write(\"\\n\".join(map(lambda x: str(x), idlist())))\n",
    "\n",
    "#important_pages.reset_index().to_json(filepath+\"filtered_syrian_pages.json\",orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Records \n",
      "1151\n",
      "Set all false\n",
      "1151\n",
      "Total pages after set relevant categories\n",
      "479\n",
      "Total pages after remove false kw and zero engagment\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from pymongo import MongoClient\n",
    "import logging,re,time\n",
    "import requests\n",
    "#import textblob\n",
    "#import langid\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 100) \n",
    "filepath = '/Users/Suad/FB-data/'\n",
    "client = MongoClient()\n",
    "db = client.unicief\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename='log.log', # log to this file\n",
    "                    format='%(asctime)s %(message)s') # include timestamp\n",
    "\n",
    "pagesCollection = db.syrpages\n",
    "postsCollection = db.syrposts\n",
    "commentsCollection = db.comments\n",
    "likesCollection=db.syrlikes\n",
    "\n",
    "\n",
    "print \"loaded Records \"\n",
    "print pagesCollection.count()\n",
    "#Set All False\n",
    "pagesCollection.update_many({},{'$set':{'relevant':False}})\n",
    "print \"Set all false\"\n",
    "print pagesCollection.count()\n",
    "\n",
    "slicee = DataFrame(list(pagesCollection.find()))\n",
    "\n",
    "#[['name','category','talking_about_count','engagement','likes','start_info_clean']]\n",
    "#print slicee\n",
    "main_cat = ['Society/Culture Website', 'Travel/Leisure' ,'Transport/Freight', 'Work Position', 'Community', \n",
    "            'Small Business',\n",
    "            'Society/Culture Website', 'Business Person', 'Business Services','Cause', 'Community Organization', \n",
    "            'Community/Government' , 'Company' , 'Consulting/Business Services', 'Regional Website', 'Media/News/Publishing'\n",
    "             ,'Local/Travel Website', 'Local Business']\n",
    "\n",
    "#false_kw =[u'عشاق','دوري','كرة','عشق','عاشق','الله','أبطال','فتيات','اسلام','ابطال','بنات'];\n",
    "false_kw =u'عشاق|دوري|كرة|عشق|عاشق|الله|أبطال|فتيات|اسلام|ابطال|بنات'\n",
    "#Loop pages\n",
    "for pid in pagesCollection.distinct(\"id\"):\n",
    "    pagesCollection.update_one({'id':pid , 'category': { '$in': main_cat } },{'$set':{'relevant':True}})\n",
    "\n",
    "print \"Total pages after set relevant categories\"\n",
    "relevant= pagesCollection.find({'relevant':True})\n",
    "print relevant.count()\n",
    "\n",
    "import re\n",
    "regexp = re.compile(false_kw)\n",
    "for page in relevant:    \n",
    "    if (regexp.search(page['name']) is not None):\n",
    "        pagesCollection.update_one({'id':page['id']} ,{'$set':{'relevant':False}})\n",
    "        #log \n",
    "        #print \"removed:\" + page['name']\n",
    "    if (page['likes']==0):\n",
    "        pagesCollection.update_one({'id':page['id']} ,{'$set':{'relevant':False}})    \n",
    "        #log \n",
    "        # print \"removed:\" + page['name']\n",
    "#for pid in relevant.distinct(\"id\"):\n",
    "\n",
    "print \"Total pages after remove false kw and zero engagment\"\n",
    "relevant= pagesCollection.find({'relevant':True})\n",
    "print relevant.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الهجرة الى اوروبا من تركيا\n",
      "378927898868694\n",
      "طرق اللجوء الى اوروبا\n",
      "152570638271216\n",
      "{u'1': [{u'length': 21, u'type': u'page', u'id': u'152570638271216', u'name': u'\\u0637\\u0631\\u0642 \\u0627\\u0644\\u0644\\u062c\\u0648\\u0621 \\u0627\\u0644\\u0649 \\u0627\\u0648\\u0631\\u0648\\u0628\\u0627', u'offset': 1}]}\n",
      "أوروبا\n",
      "776373632471822\n",
      "{u'1': [{u'length': 6, u'type': u'page', u'id': u'776373632471822', u'name': u'\\u0623\\u0648\\u0631\\u0648\\u0628\\u0627', u'offset': 1}]}\n",
      "{u'1': [{u'length': 6, u'type': u'page', u'id': u'776373632471822', u'name': u'\\u0623\\u0648\\u0631\\u0648\\u0628\\u0627', u'offset': 1}]}\n",
      "اخبار المهاجرين في اوروبا\n",
      "307091372652912\n",
      "{u'1': [{u'length': 25, u'type': u'page', u'id': u'307091372652912', u'name': u'\\u0627\\u062e\\u0628\\u0627\\u0631 \\u0627\\u0644\\u0645\\u0647\\u0627\\u062c\\u0631\\u064a\\u0646 \\u0641\\u064a \\u0627\\u0648\\u0631\\u0648\\u0628\\u0627', u'offset': 1}]}\n",
      "اوروبا مستقبلي\n",
      "449543831789061\n",
      "{u'1': [{u'length': 14, u'type': u'page', u'id': u'449543831789061', u'name': u'\\u0627\\u0648\\u0631\\u0648\\u0628\\u0627 \\u0645\\u0633\\u062a\\u0642\\u0628\\u0644\\u064a', u'offset': 1}]}\n",
      "{u'1': [{u'length': 14, u'type': u'page', u'id': u'449543831789061', u'name': u'\\u0627\\u0648\\u0631\\u0648\\u0628\\u0627 \\u0645\\u0633\\u062a\\u0642\\u0628\\u0644\\u064a', u'offset': 1}]}\n"
     ]
    }
   ],
   "source": [
    "#loop relevant \n",
    "limit=0\n",
    "for page in relevant: \n",
    "    limit +=1\n",
    "    if(limit>5):\n",
    "        break\n",
    "    print page['name']\n",
    "    print page['id']\n",
    "    #g.add_vertices(page[\"id\"])\n",
    "    # get page posts \n",
    "    posts = postsCollection.find({\"page_id\":page['id']})\n",
    "    limitp =0\n",
    "    for post in posts:\n",
    "        limitp+=1\n",
    "        if(limitp >5):\n",
    "            break\n",
    "        \n",
    "        if \"story_tags\" in post:\n",
    "            print post['story_tags']\n",
    "            \n",
    "        \n",
    "        #g.add_vertices(post[\"id\"])\n",
    "    # get users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "3590460\n",
      "317666\n",
      "124232\n",
      "posts: set([u'916807138406205'])\n",
      "users: set([u'140724502942173', u'724151681050049', u'712707625529964', u'701472406650308', u'727409774060908', u'1641450669473438', u'1521603274798444', u'511741552335864', u'506190092877137', u'140119983003128'])\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "limit =0\n",
    "result =[]\n",
    "relevant= pagesCollection.find({'relevant':True},{\"id\":1, \"_id\":0})\n",
    "\n",
    "#relevant.toArrays(function(u){ result.push(u.text) })\n",
    "x = 449543831789061\n",
    "print result\n",
    "B = nx.Graph()\n",
    "usersGraph = nx.Graph()\n",
    "relevant_pages=[]\n",
    "posts =[]\n",
    "net={}\n",
    "for x in relevant:\n",
    "    relevant_pages.append(x[\"id\"]) # find better way todo this\n",
    "    \n",
    "print likesCollection.count()\n",
    "print commentsCollection.count()\n",
    "print postsCollection.count()\n",
    "for comment in commentsCollection.find().limit(10):\n",
    "    \n",
    "    ids = comment['id'].split('_')\n",
    "    key = post_id = ids[0]\n",
    "    #p= postsCollection.find({'id':key},{\"page_id\":1, \"_id\":0})\n",
    "    #print list(p)\n",
    "    #post_id =ids[1]\n",
    "    user = comment['from']['id']\n",
    "    if key not in net:\n",
    "        net[key] = [user]\n",
    "    else:\n",
    "        #print \"found\"\n",
    "        if user not in net[key]:\n",
    "            net[key].append(user)\n",
    "\n",
    "B.add_nodes_from(net.keys(),Bipartite=0)\n",
    "        \n",
    "for post in net:\n",
    "        B.add_nodes_from(list(net[post]),Bipartite=1)\n",
    "        for value in net[post]:\n",
    "            B.add_edge(post,value)\n",
    "            \n",
    "users_nodes, posts_nodes = bipartite.sets(B)\n",
    "print \"posts:\",posts_nodes\n",
    "print \"users:\",users_nodes\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 4), (1, 'b'), (2, 'c'), (2, 'b'), (3, 'c')]\n",
      "['a', 1, 2, 3, 4, 'c', 'b']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from([1,2,3,4], bipartite=0) # Add the node attribute \"bipartite\"\n",
    "B.add_nodes_from(['a','b','c'], bipartite=1)\n",
    "B.add_edges_from([(1,'a'), (1,'b'), (2,'b'), (2,'c'), (3,'c'), (4,'a')])\n",
    "print B.edges()\n",
    "print B.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}